{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "834dd632",
   "metadata": {},
   "source": [
    "# Challenge Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359edaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import vertexai\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path.cwd().parent / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename=\"app.log\", level=logging.INFO)\n",
    "\n",
    "GCP_PROJECT = os.getenv(\"GCP_PROJECT\")\n",
    "GCP_REGION = os.getenv(\"GCP_REGION\")\n",
    "GOOGLE_MAP_KEY = os.getenv(\"GOOGLE_MAP_KEY\")\n",
    "\n",
    "vertexai.init(project=GCP_PROJECT, location=GCP_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23dc0",
   "metadata": {},
   "source": [
    "### Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a46b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_tools import get_lat_lon_from_address, get_weather_forecast\n",
    "\n",
    "\n",
    "def get_weather(address: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"Takes an address and returns a weather forcast from National Weather Service.\n",
    "\n",
    "    Args:\n",
    "        address: The street address or place name to geocode (e.g., \"1600 Amphitheatre Parkway, Mountain View, CA\").\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a forecast\n",
    "        period (e.g., 'Tonight', 'Thursday'). Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lat, lon = get_lat_lon_from_address(address=address, api_key=GOOGLE_MAP_KEY)\n",
    "        forecast = get_weather_forecast(lat, lon)\n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        print(f\"Something broke. Good luck fixing:\\n{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647940e",
   "metadata": {},
   "source": [
    "### Agent Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e51715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmRequest, LlmResponse\n",
    "\n",
    "from agent_tools import is_address_in_us, is_user_query_mean\n",
    "\n",
    "\n",
    "def user_prompt_log_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Logs the content of the user's latest prompt.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_request: The request object sent to the LLM.\n",
    "\n",
    "    Returns:\n",
    "        LlmResponse or None.\n",
    "    \"\"\"\n",
    "    if llm_request.contents:\n",
    "        last = llm_request.contents[-1]\n",
    "        if last.role == \"user\" and last.parts and last.parts[0].text:\n",
    "            user_text = last.parts[0].text.strip()\n",
    "            logger.info(f\"[{callback_context.agent_name}] USER >> {user_text}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def model_response_log_callback(\n",
    "    callback_context: CallbackContext, llm_response: LlmResponse\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Logs the content of the model's response.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_response: The response object received from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        LlmResponse or None.\n",
    "    \"\"\"\n",
    "    if llm_response.content and llm_response.content.parts:\n",
    "        txt = llm_response.content.parts[0].text\n",
    "        if txt:\n",
    "            logger.info(f\"[{callback_context.agent_name}] MODEL >> {txt.strip()}\")\n",
    "\n",
    "\n",
    "def user_query_check_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Performs moderation checks on the user's query.\n",
    "\n",
    "    Checks for non-US addresses and harmful content. If a check fails,\n",
    "    it returns a pre-canned LlmResponse to stop further processing.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_request: The request object containing the user's query.\n",
    "\n",
    "    Returns:\n",
    "        An LlmResponse to short-circuit the chain if moderation fails,\n",
    "        otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        last = llm_request.contents[-1]\n",
    "\n",
    "        if last.role == \"user\" and last.parts and last.parts[0].text:\n",
    "            user_text = last.parts[0].text.strip()\n",
    "            if not is_address_in_us(\n",
    "                project_id=GCP_PROJECT, location=GCP_REGION, user_query=user_text\n",
    "            ):\n",
    "                return LlmResponse(\n",
    "                    content={\n",
    "                        \"role\": \"model\",\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Message contains non-US addresses, \"\n",
    "                                \"please only query for US addresses.\"\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if is_user_query_mean(\n",
    "                project_id=GCP_PROJECT, location=GCP_REGION, user_query=user_text\n",
    "            ):\n",
    "                return LlmResponse(\n",
    "                    content={\"role\": \"model\", \"parts\": [{\"text\": \"Be nice.\"}]}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Woops:\\n{e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def chained_before_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"A chained 'before' callback that runs multiple checks in sequence.\n",
    "\n",
    "    It first runs a moderation check. If the moderation check returns a\n",
    "    response, this function immediately returns it. Otherwise, it proceeds\n",
    "    to log the user's input.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_request: The request object to be processed.\n",
    "\n",
    "    Returns:\n",
    "        An LlmResponse if moderation fails, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Moderation check\n",
    "    moderation_result = user_query_check_callback(callback_context, llm_request)\n",
    "    if moderation_result is not None:\n",
    "        return moderation_result\n",
    "\n",
    "    # 2. Log user input\n",
    "    user_prompt_log_callback(callback_context, llm_request)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5186f",
   "metadata": {},
   "source": [
    "### Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3ab6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from vertexai.preview import reasoning_engines\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "weather_agent_with_moderation = LlmAgent(\n",
    "    name=\"weather_agent_with_moderation\",\n",
    "    model=\"gemini-2.0-flash\", # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], \n",
    "#    before_model_callback=chained_before_callback,\n",
    "#    after_model_callback=model_response_log_callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16d37b",
   "metadata": {},
   "source": [
    "### Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ecb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.tools import google_search, agent_tool\n",
    "\n",
    "google_search_agent = LlmAgent(\n",
    "    name=\"google_search_agent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    description=\"A helpful agent that can search the web for information.\",\n",
    "    instruction=\"You are a helpful assistant that can search the web for information. \"\n",
    "                \"When the user asks a question, use the 'google_search' tool to find the answer.\",\n",
    "    tools=[google_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1504b7",
   "metadata": {},
   "source": [
    "### Main Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b12eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_agent = LlmAgent(\n",
    "    name=\"main_agent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    description=\"A helpful agent that can answer questions and use tools.\",\n",
    "    instruction=\"You are a helpful assistant that can answer questions and use tools. \"\n",
    "                \"If the user asks about the weather, use the 'weather_agent_with_moderation' tool. \"\n",
    "                \"If the user asks a general question, use the 'google_search_agent' tool.\",\n",
    "    tools=[agent_tool.AgentTool(agent=google_search_agent)],\n",
    "    sub_agents=[weather_agent_with_moderation]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f57ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the weather forecast for New York for the next three days:\n",
       "\n",
       "| Day             | Forecast                                                                                                                                | Temperature |\n",
       "|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------|-------------|\n",
       "| **This Afternoon**| A slight chance of showers and thunderstorms between 2pm and 5pm, then a chance of showers and thunderstorms. Mostly sunny.           | High near 86F   |\n",
       "| **Tonight**       | Showers and thunderstorms. Mostly cloudy.                                                                                              | Low around 75F    |\n",
       "| **Thursday**      | A chance of rain showers before 2pm, then a chance of showers and thunderstorms. Mostly cloudy.                                       | High near 79F   |\n",
       "| **Thursday Night**| A chance of showers and thunderstorms before 8pm, then a chance of showers and thunderstorms between 8pm and 2am. Mostly cloudy.        | Low around 74F    |\n",
       "| **Friday**        | A slight chance of rain showers between 8am and 2pm, then a chance of showers and thunderstorms. Mostly cloudy.                        | High near 81F   |\n",
       "| **Friday Night**  | A chance of showers and thunderstorms before 8pm, then a slight chance of showers and thunderstorms. Mostly cloudy.                   | Low around 73F    |\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = reasoning_engines.AdkApp(\n",
    "    agent=main_agent,\n",
    "    enable_tracing=False,\n",
    ")\n",
    "\n",
    "user_id = \"test-user-id\"\n",
    "session = app.create_session(user_id=user_id)\n",
    "\n",
    "for event in app.stream_query(\n",
    "    user_id=user_id,\n",
    "    session_id = session.id,\n",
    "    message=\"What's the weather in New York for the next three days? Provide response as markdown table.\",\n",
    "):\n",
    "  lastevent = event\n",
    "\n",
    "display(Markdown(lastevent[\"content\"][\"parts\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671aa3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A hot dog is a cooked sausage, often a frankfurter or wiener, that is typically served in a long, soft roll. It can be grilled, steamed, or boiled. Common condiments include mustard, ketchup, relish, and onions. \"Hot dog\" can also refer to someone who performs in a conspicuous or ostentatious manner, especially by performing fancy stunts, or be used as an interjection to express approval or pleasure.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for event in app.stream_query(\n",
    "    user_id=user_id,\n",
    "    session_id = session.id,\n",
    "    message=\"What's a hot dog?\",\n",
    "):\n",
    "  lastevent = event\n",
    "\n",
    "display(Markdown(lastevent[\"content\"][\"parts\"][0][\"text\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
