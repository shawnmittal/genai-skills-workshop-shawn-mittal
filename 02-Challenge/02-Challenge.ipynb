{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "834dd632",
   "metadata": {},
   "source": [
    "# Challenge Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359edaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import vertexai\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path.cwd().parent / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename=\"app.log\", level=logging.INFO)\n",
    "\n",
    "GCP_PROJECT = os.getenv(\"GCP_PROJECT\")\n",
    "GCP_REGION = os.getenv(\"GCP_REGION\")\n",
    "GOOGLE_MAP_KEY = os.getenv(\"GOOGLE_MAP_KEY\")\n",
    "\n",
    "vertexai.init(project=GCP_PROJECT, location=GCP_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23dc0",
   "metadata": {},
   "source": [
    "### Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a46b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_tools import get_lat_lon_from_address, get_weather_forecast\n",
    "\n",
    "\n",
    "def get_weather(address: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"Takes an address and returns a weather forcast from National Weather Service.\n",
    "\n",
    "    Args:\n",
    "        address: The street address or place name to geocode (e.g., \"1600 Amphitheatre Parkway, Mountain View, CA\").\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a forecast\n",
    "        period (e.g., 'Tonight', 'Thursday'). Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lat, lon = get_lat_lon_from_address(address=address, api_key=GOOGLE_MAP_KEY)\n",
    "        forecast = get_weather_forecast(lat, lon)\n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        print(f\"Something broke. Good luck fixing:\\n{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647940e",
   "metadata": {},
   "source": [
    "### Agent Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e51715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmRequest, LlmResponse\n",
    "\n",
    "from agent_tools import is_address_in_us, is_user_query_mean\n",
    "\n",
    "\n",
    "def user_prompt_log_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Logs the content of the user's latest prompt.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_request: The request object sent to the LLM.\n",
    "\n",
    "    Returns:\n",
    "        LlmResponse or None.\n",
    "    \"\"\"\n",
    "    if llm_request.contents:\n",
    "        last = llm_request.contents[-1]\n",
    "        if last.role == \"user\" and last.parts and last.parts[0].text:\n",
    "            user_text = last.parts[0].text.strip()\n",
    "            logger.info(f\"[{callback_context.agent_name}] USER >> {user_text}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def model_response_log_callback(\n",
    "    callback_context: CallbackContext, llm_response: LlmResponse\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Logs the content of the model's response.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_response: The response object received from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        LlmResponse or None.\n",
    "    \"\"\"\n",
    "    if llm_response.content and llm_response.content.parts:\n",
    "        txt = llm_response.content.parts[0].text\n",
    "        if txt:\n",
    "            logger.info(f\"[{callback_context.agent_name}] MODEL >> {txt.strip()}\")\n",
    "\n",
    "\n",
    "def user_query_check_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Performs moderation checks on the user's query.\n",
    "\n",
    "    Checks for non-US addresses and harmful content. If a check fails,\n",
    "    it returns a pre-canned LlmResponse to stop further processing.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_request: The request object containing the user's query.\n",
    "\n",
    "    Returns:\n",
    "        An LlmResponse to short-circuit the chain if moderation fails,\n",
    "        otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        last = llm_request.contents[-1]\n",
    "\n",
    "        if last.role == \"user\" and last.parts and last.parts[0].text:\n",
    "            user_text = last.parts[0].text.strip()\n",
    "            if not is_address_in_us(\n",
    "                project_id=GCP_PROJECT, location=GCP_REGION, user_query=user_text\n",
    "            ):\n",
    "                return LlmResponse(\n",
    "                    content={\n",
    "                        \"role\": \"model\",\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Message contains non-US addresses, \"\n",
    "                                \"please only query for US addresses.\"\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if is_user_query_mean(\n",
    "                project_id=GCP_PROJECT, location=GCP_REGION, user_query=user_text\n",
    "            ):\n",
    "                return LlmResponse(\n",
    "                    content={\"role\": \"model\", \"parts\": [{\"text\": \"Be nice.\"}]}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Woops:\\n{e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def chained_before_callback(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"A chained 'before' callback that runs multiple checks in sequence.\n",
    "\n",
    "    It first runs a moderation check. If the moderation check returns a\n",
    "    response, this function immediately returns it. Otherwise, it proceeds\n",
    "    to log the user's input.\n",
    "\n",
    "    Args:\n",
    "        callback_context: The context of the agent executing the callback.\n",
    "        llm_request: The request object to be processed.\n",
    "\n",
    "    Returns:\n",
    "        An LlmResponse if moderation fails, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Moderation check\n",
    "    moderation_result = user_query_check_callback(callback_context, llm_request)\n",
    "    if moderation_result is not None:\n",
    "        return moderation_result\n",
    "\n",
    "    # 2. Log user input\n",
    "    user_prompt_log_callback(callback_context, llm_request)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5186f",
   "metadata": {},
   "source": [
    "### Agent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3ab6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the weather forecast for the next three days in New York, Seattle, and Chicago:\n",
       "\n",
       "| City    | Day         | Forecast                                                                | Temperature |\n",
       "|---------|-------------|--------------------------------------------------------------------------|-------------|\n",
       "| New York| Today       | Slight Chance Showers And Thunderstorms                                  | 86°F        |\n",
       "|         | Tonight     | Showers And Thunderstorms                                               | 75°F        |\n",
       "|         | Thursday    | Chance Rain Showers then Chance Showers And Thunderstorms                 | 79°F        |\n",
       "| Seattle | Today       | Chance Light Rain                                                       | 71°F        |\n",
       "|         | Tonight     | Chance Light Rain then Cloudy                                          | 57°F        |\n",
       "|         | Thursday    | Partly Sunny                                                              | 74°F        |\n",
       "| Chicago | Today       | Isolated Showers And Thunderstorms                                        | 78°F        |\n",
       "|         | Tonight     | Partly Cloudy                                                              | 69°F        |\n",
       "|         | Thursday    | Mostly Sunny                                                              | 78°F        |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from vertexai.preview import reasoning_engines\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "weather_agent_with_moderation = LlmAgent(\n",
    "    name=\"weather_agent_v2\",\n",
    "    model=\"gemini-2.0-flash\", # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], \n",
    "    before_model_callback=chained_before_callback,\n",
    "    after_model_callback=model_response_log_callback\n",
    ")\n",
    "\n",
    "\n",
    "app = reasoning_engines.AdkApp(\n",
    "    agent=weather_agent_with_moderation,\n",
    "    enable_tracing=False,\n",
    ")\n",
    "\n",
    "user_id = \"test-user-id\"\n",
    "session = app.create_session(user_id=user_id)\n",
    "\n",
    "for event in app.stream_query(\n",
    "    user_id=user_id,\n",
    "    session_id = session.id,\n",
    "    message=\"What's the weather in New York, Seattle, and Chicago for the next three days? Provide response as markdown table.\",\n",
    "):\n",
    "  lastevent = event\n",
    "\n",
    "display(Markdown(lastevent[\"content\"][\"parts\"][0][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f4357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Be nice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for event in app.stream_query(\n",
    "    user_id=user_id,\n",
    "    session_id = session.id,\n",
    "    message=\"You're a terrible bot and you smell of elderberries. What's the weather in Seattle and Chicago for the next three days? Provide response as markdown table.\",\n",
    "):\n",
    "  lastevent = event\n",
    "\n",
    "display(Markdown(lastevent[\"content\"][\"parts\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09a619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Message contains non-US addresses, please only query for US addresses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for event in app.stream_query(\n",
    "    user_id=user_id,\n",
    "    session_id = session.id,\n",
    "    message=\"What's the weather in Paris, France, Seattle, and Chicago for the next three days? Provide response as markdown table.\",\n",
    "):\n",
    "  lastevent = event\n",
    "\n",
    "display(Markdown(lastevent[\"content\"][\"parts\"][0][\"text\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
